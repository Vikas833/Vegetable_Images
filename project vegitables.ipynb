{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc2571b-110a-4a2a-8eb2-ef896aa4248d",
   "metadata": {},
   "source": [
    "<h1>Vegitable Name Recognization Project</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e2caaa-db65-458b-8559-094cab213196",
   "metadata": {},
   "source": [
    "<h5>The Vegetable Name Recognition Project leverages the power of neural networks to accurately identify vegetables from an image dataset. With the growing need for efficient food classification in various fields, such as smart kitchens, agriculture, and supply chain management, this project focuses on recognizing 15 commonly used vegetables:\n",
    "<br>\n",
    "<br>\n",
    "    Bean, Bitter Gourd, Bottle Gourd, Brinjal, Broccoli, Cabbage, Capsicum, Carrot, Cauliflower, Cucumber, Papaya, Potato, Pumpkin, Radish, and Tomato.\n",
    "<br>\n",
    "<br>\n",
    "The project utilizes a convolutional neural network (CNN), a deep learning architecture particularly suited for image recognition tasks. By training the model on a curated dataset of vegetable images, the system learns to extract key visual features such as shape, texture, and color to distinguish between these vegetables.\n",
    "<br>\n",
    "<br>\n",
    "This application has potential uses in automated grocery checkout systems, meal preparation assistants, and educational tools, providing an efficient solution for vegetable identification from images.</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8bc8b-5ca6-483d-b7c3-ea95a853fa41",
   "metadata": {},
   "source": [
    "<h3>Importing Important Python Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3325de0f-e9a9-4b6b-97d7-2635d77ca742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Dense, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from PIL import Image, ImageOps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image, ImageOps \n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f11a0-3a59-4a7f-9215-57b8f65095ef",
   "metadata": {},
   "source": [
    "<h3>Assigning The directories</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f268ca-9a26-4ae9-b7aa-bdfd9e77bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir1 = 'train'\n",
    "img_dir2 = \"test\"\n",
    "batch_size = 64\n",
    "img_size = 150\n",
    "input_shape = (150, 150, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e0d19-ce8c-41cb-a719-6e2524762811",
   "metadata": {},
   "source": [
    "<h3>Importing The Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9743934-c916-45c0-8d4b-d256f8ef1d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12000 images belonging to 15 classes.\n",
      "Found 3000 images belonging to 15 classes.\n",
      "Found 3000 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "data_gen = ImageDataGenerator(rescale = 1/255, validation_split=0.2)\n",
    "data_gen_test = ImageDataGenerator(rescale=1/255)\n",
    "     \n",
    "train_data = data_gen.flow_from_directory(img_dir1, target_size=(img_size, img_size),subset=\"training\", color_mode=\"grayscale\", shuffle=True, class_mode=\"binary\")\n",
    "val_data = data_gen.flow_from_directory(img_dir1, target_size=(img_size, img_size),subset=\"validation\", color_mode=\"grayscale\", shuffle=True, class_mode=\"binary\")\n",
    "test_data = data_gen_test.flow_from_directory(img_dir2, target_size=(img_size, img_size), color_mode=\"grayscale\", shuffle=False, batch_size=batch_size, class_mode=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae4a73e-d3a2-4cb3-a92b-4eac49ad4552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bean',\n",
       " 'Bitter_Gourd',\n",
       " 'Bottle_Gourd',\n",
       " 'Brinjal',\n",
       " 'Broccoli',\n",
       " 'Cabbage',\n",
       " 'Capsicum',\n",
       " 'Carrot',\n",
       " 'Cauliflower',\n",
       " 'Cucumber',\n",
       " 'Papaya',\n",
       " 'Potato',\n",
       " 'Pumpkin',\n",
       " 'Radish',\n",
       " 'Tomato']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train_data.class_indices\n",
    "classes = list(labels.keys())\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dd5663-7753-4887-9b59-f7051745fae4",
   "metadata": {},
   "source": [
    "<h3>Make the Convolutional Nueral Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa744bf1-b4cf-4cbc-a0b8-67cbfab475c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "cnn = models.Sequential([\n",
    "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(15, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efe243c-12cb-4adc-8aed-8fe9947d9fa5",
   "metadata": {},
   "source": [
    "<h3>Compiling the Nueral Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50145450-7e99-41ab-a54e-f933a3e2a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e27b1-82cf-4147-84f7-a0452ac0a2a8",
   "metadata": {},
   "source": [
    "<h3>Compiling the Nueral Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feb2d918-7738-499b-a4af-573755761fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 421ms/step - accuracy: 0.9929 - loss: 0.0250\n",
      "Epoch 2/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 535ms/step - accuracy: 0.9889 - loss: 0.0367\n",
      "Epoch 3/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 419ms/step - accuracy: 0.9954 - loss: 0.0169\n",
      "Epoch 4/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 401ms/step - accuracy: 0.9801 - loss: 0.0654\n",
      "Epoch 5/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 395ms/step - accuracy: 0.9963 - loss: 0.0153\n",
      "Epoch 6/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 390ms/step - accuracy: 0.9910 - loss: 0.0272\n",
      "Epoch 7/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 392ms/step - accuracy: 0.9873 - loss: 0.0415\n",
      "Epoch 8/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 397ms/step - accuracy: 0.9961 - loss: 0.0117\n",
      "Epoch 9/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 390ms/step - accuracy: 0.9830 - loss: 0.0527\n",
      "Epoch 10/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 389ms/step - accuracy: 0.9933 - loss: 0.0222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x17237a44440>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(train_data, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbed09-bf2a-41d7-a0fd-cd1c66fa69da",
   "metadata": {},
   "source": [
    "<h3>Evaluating the Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "308ea66c-4f55-49d6-abb6-f5e49e22bd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 151ms/step - accuracy: 0.9996 - loss: 0.0026\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 152ms/step - accuracy: 0.9992 - loss: 0.0028\n",
      "Train data loss is 0.002363087609410286, Train data Accuracy 0.9995833039283752, Test data loss is 0.0023630885407328606, Test data Accuracy is 0.9995833039283752\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc = cnn.evaluate(train_data)\n",
    "test_loss, test_acc = cnn.evaluate(train_data)\n",
    "print(f\"Train data loss is {train_loss}, Train data Accuracy {train_acc}, Test data loss is {test_loss}, Test data Accuracy is {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "def0306e-91a4-4d02-897d-66d726e39a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.ndarray(shape=(1, 150, 150, 1), dtype=np.float32)\n",
    "# image = Image.open(\"test\\Bitter_Gourd\\1233 .jpg\")\n",
    "# size = (150, 150)\n",
    "# image = ImageOps.grayscale(image)\n",
    "# image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)\n",
    "# image_array = np.array(image)\n",
    "# display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb60db9-a9aa-4c7f-952e-da6e4b6abc90",
   "metadata": {},
   "source": [
    "<h3>Plotting Graph of Training and validation accuracy and loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cd09d5b-0d6e-463a-b2c5-6c3fa675b523",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 25\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# loss = cnn.history['loss']\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# val_loss = cnn.history['val_loss']\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# epochs = range(1, len(loss) + 1)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# plt.legend()\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     26\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m cnn\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     27\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(loss) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "# loss = cnn.history['loss']\n",
    "# val_loss = cnn.history['val_loss']\n",
    "# epochs = range(1, len(loss) + 1)\n",
    "# plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# acc = cnn.history['accuracy']\n",
    "# val_acc = cnn.history['val_accuracy']\n",
    "\n",
    "# plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "loss = cnn.history.history['loss']\n",
    "val_loss = cnn.history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = cnn.history.history['accuracy']\n",
    "val_acc = cnn.history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6a3ac8-47d7-49ea-825f-d651f8124f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
